"""
æœ€å°åŒ–çš„Fairyæ‰§è¡Œå™¨ - ç‹¬ç«‹ç‰ˆæœ¬
åŠŸèƒ½ï¼šå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„ä¸ºå…·ä½“çš„ç§»åŠ¨ç«¯æ“ä½œå¹¶æ‰§è¡Œ

æ ¸å¿ƒæµç¨‹ï¼š
1. è·å–å±å¹•ä¿¡æ¯ï¼ˆæˆªå›¾ + UIç»“æ„ï¼‰
2. LLMå†³ç­–å…·ä½“åŠ¨ä½œï¼ˆåŸºäºæŒ‡ä»¤å’Œå±å¹•ä¿¡æ¯ï¼‰
3. æ‰§è¡ŒåŠ¨ä½œ
4. è¿”å›æ‰§è¡Œç»“æœ

ä¸ä¾èµ–ï¼š
- Citlaliæ¡†æ¶
- Memoryç³»ç»Ÿï¼ˆé€šè¿‡å‚æ•°ä¼ é€’ï¼‰
- RAGç³»ç»Ÿï¼ˆå¯é€‰ï¼‰
"""

import asyncio
import json
import os
import re
from dataclasses import dataclass
from typing import List, Dict, Optional

from dotenv import load_dotenv

from Citlali.models.entity import ChatMessage
from Citlali.models.openai.client import OpenAIChatClient
from Fairy.config.model_config import ModelConfig
from Fairy.entity.info_entity import ScreenInfo, ActionInfo, PlanInfo
from Fairy.tools.mobile_controller.action_type import AtomicActionType, ATOMIC_ACTION_SIGNITURES
# Fairyæ ¸å¿ƒä¾èµ–
from Fairy.tools.mobile_controller.ui_automator_tools.mobile_control_tool import UiAutomatorMobileController
from Fairy.tools.mobile_controller.ui_automator_tools.screen_capture_tool import UiAutomatorMobileScreenCapturer
from Fairy.tools.screen_perceptor.ssip_new.perceptor.perceptor import ScreenStructuredInfoPerception


# ==================== é…ç½®ç±» ====================

@dataclass
class MinimalExecutorConfig:
    """æœ€å°åŒ–æ‰§è¡Œå™¨é…ç½®"""
    device: str  # è®¾å¤‡ID
    model_client: any  # LLMå®¢æˆ·ç«¯ï¼ˆå…¼å®¹Fairyçš„æ¥å£ï¼‰

    # å¯é€‰é…ç½®
    temp_path: str = "../../tmp"
    screenshot_phone_path: str = "/sdcard"
    screenshot_filename: str = "screenshot"

    # å±å¹•æ„ŸçŸ¥é…ç½®ï¼ˆå¯é€‰ï¼‰
    visual_prompt_model_config: Optional[any] = None
    text_summarization_model_config: Optional[any] = None
    non_visual_mode: bool = False  # True=çº¯æ–‡æœ¬æ¨¡å¼ï¼ŒFalse=ä½¿ç”¨æ ‡è®°å›¾åƒ


# ==================== æ‰§è¡Œç»“æœç±» ====================

@dataclass
class ExecutionResult:
    """æ‰§è¡Œç»“æœ"""
    success: bool
    actions_taken: List[Dict]
    action_thought: str
    action_expectation: str
    screen_before: ScreenInfo
    screen_after: Optional[ScreenInfo] = None
    error: Optional[str] = None


# ==================== æ ¸å¿ƒæ‰§è¡Œå™¨ ====================

class MinimalFairyExecutor:
    """
    æœ€å°åŒ–çš„Fairyæ‰§è¡Œå™¨

    åŠŸèƒ½ï¼šå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºå…·ä½“çš„ç§»åŠ¨ç«¯æ“ä½œå¹¶æ‰§è¡Œ

    ç¤ºä¾‹ï¼š
        executor = MinimalFairyExecutor(
            device="emulator-5554",
            model_client=your_model_client
        )

        result = await executor.execute_instruction(
            instruction="åœ¨å½“å‰å¤–å–ç•Œé¢ç‚¹å‡»è´­ç‰©è½¦",
            plan_context={
                "overall_plan": "æµè§ˆå•†å“å¹¶æŸ¥çœ‹è´­ç‰©è½¦",
                "current_sub_goal": "ç‚¹å‡»è´­ç‰©è½¦"
            }
        )

        print(f"æˆåŠŸ: {result.success}")
        print(f"æ‰§è¡Œçš„åŠ¨ä½œ: {result.actions_taken}")
    """

    def __init__(self, device: str, model_client: any, config: Optional[MinimalExecutorConfig] = None):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            device: è®¾å¤‡ID
            model_client: LLMå®¢æˆ·ç«¯ï¼Œéœ€è¦å®ç° async create(messages) æ–¹æ³•
            config: å¯é€‰çš„è¯¦ç»†é…ç½®
        """
        if config is None:
            config = MinimalExecutorConfig(device=device, model_client=model_client)
        else:
            config.device = device
            config.model_client = model_client

        self.config = config
        self.model_client = model_client

        # åˆå§‹åŒ–åº•å±‚å·¥å…·
        fairy_config = self._create_fairy_config()
        self.controller = UiAutomatorMobileController(fairy_config)
        self.screen_capturer = UiAutomatorMobileScreenCapturer(fairy_config)

        # åˆå§‹åŒ–å±å¹•æ„ŸçŸ¥å™¨ï¼ˆå¦‚æœæä¾›äº†æ¨¡å‹é…ç½®ï¼‰
        if config.visual_prompt_model_config is not None or config.non_visual_mode:
            self.screen_perceptor = ScreenStructuredInfoPerception(
                config.visual_prompt_model_config,
                config.text_summarization_model_config
            )
        else:
            self.screen_perceptor = None

    def _create_fairy_config(self):
        """åˆ›å»ºFairyé…ç½®å¯¹è±¡ï¼ˆMockï¼‰"""
        class MockFairyConfig:
            def __init__(self, config: MinimalExecutorConfig):
                self.device = config.device
                self.temp_path = config.temp_path
                self.screenshot_phone_path = config.screenshot_phone_path
                self.screenshot_filename = config.screenshot_filename
                self.task_temp_path = config.temp_path

            def get_screenshot_temp_path(self):
                return self.temp_path

        return MockFairyConfig(self.config)

    # ==================== ä¸»è¦æ¥å£ ====================

    async def execute_instruction(
        self,
        instruction: str,
        plan_context: Optional[Dict] = None,
        historical_actions: Optional[List[Dict]] = None,
        execution_tips: str = "",
        key_infos: Optional[List] = None,
        language: str = "Chinese"
    ) -> ExecutionResult:
        """
        æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤

        Args:
            instruction: è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¦‚ "åœ¨å½“å‰å¤–å–ç•Œé¢ç‚¹å‡»è´­ç‰©è½¦"
            plan_context: è®¡åˆ’ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«:
                - overall_plan: æ•´ä½“è®¡åˆ’
                - current_sub_goal: å½“å‰å­ç›®æ ‡
            historical_actions: å†å²åŠ¨ä½œåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            execution_tips: æ‰§è¡Œå»ºè®®ï¼ˆå¯é€‰ï¼Œå¯ä»¥ä»RAGè·å–ï¼‰
            key_infos: å…³é”®ä¿¡æ¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            language: æŒ‡ä»¤è¯­è¨€

        Returns:
            ExecutionResult: æ‰§è¡Œç»“æœ
        """
        print(f"\nğŸš€ [DEBUG] å¼€å§‹æ‰§è¡ŒæŒ‡ä»¤: {instruction}")
        print(f"   è®¡åˆ’ä¸Šä¸‹æ–‡: {plan_context}")
        print(f"   æ‰§è¡Œå»ºè®®: {execution_tips}")

        try:
            # 1. è·å–å±å¹•ä¿¡æ¯
            print(f"   æ­¥éª¤1: è·å–å±å¹•ä¿¡æ¯...")
            screen_before = await self._get_screen_info()
            print(f"   âœ… å±å¹•ä¿¡æ¯è·å–æˆåŠŸ")

            # 2. æ„å»ºè®¡åˆ’ä¿¡æ¯
            print(f"   æ­¥éª¤2: æ„å»ºè®¡åˆ’ä¿¡æ¯...")
            if plan_context is None:
                plan_context = {
                    "overall_plan": instruction,
                    "current_sub_goal": instruction
                }

            plan_info = PlanInfo(
                plan_thought="",
                overall_plan=plan_context.get("overall_plan", instruction),
                current_sub_goal=plan_context.get("current_sub_goal", instruction),
                user_interaction_type="0",
                user_interaction_thought=""
            )
            print(f"   âœ… è®¡åˆ’ä¿¡æ¯æ„å»ºæˆåŠŸ")

            # 3. LLMå†³ç­–åŠ¨ä½œ
            print(f"   æ­¥éª¤3: è°ƒç”¨LLMå†³ç­–åŠ¨ä½œ...")
            action_info = await self._decide_action(
                instruction=instruction,
                language=language,
                plan_info=plan_info,
                screen_info=screen_before,
                historical_actions=historical_actions or [],
                execution_tips=execution_tips,
                key_infos=key_infos or []
            )

            if action_info is None:
                return ExecutionResult(
                    success=False,
                    actions_taken=[],
                    action_thought="",
                    action_expectation="",
                    screen_before=screen_before,
                    error="Failed to decide action"
                )

            # 4. æ‰§è¡ŒåŠ¨ä½œ
            await self._execute_actions(action_info.actions)

            # 5. è·å–æ‰§è¡Œåçš„å±å¹•ä¿¡æ¯
            screen_after = await self._get_screen_info()

            return ExecutionResult(
                success=True,
                actions_taken=action_info.actions,
                action_thought=action_info.action_thought,
                action_expectation=action_info.action_expectation,
                screen_before=screen_before,
                screen_after=screen_after
            )

        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            print(f"\nâŒ [ERROR] æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸:")
            print(error_traceback)
            return ExecutionResult(
                success=False,
                actions_taken=[],
                action_thought="",
                action_expectation="",
                screen_before=screen_before if 'screen_before' in locals() else None,
                error=str(e)
            )

    # ==================== å†…éƒ¨æ–¹æ³• ====================

    async def _get_screen_info(self) -> ScreenInfo:
        """è·å–å±å¹•ä¿¡æ¯"""
        # è·å–å½“å‰Activity
        activity_info = await self.screen_capturer.get_current_activity()

        # è·å–æˆªå›¾å’ŒUIå±‚æ¬¡ç»“æ„
        screenshot_file_info, ui_hierarchy_xml = await self.screen_capturer.get_screen()
        screenshot_file_info.compress_image_to_jpeg()

        # è·å–é”®ç›˜çŠ¶æ€
        keyboard_status = await self.screen_capturer.get_keyboard_activation_status()

        # è§£æå±å¹•ï¼ˆå¦‚æœé…ç½®äº†æ„ŸçŸ¥å™¨ï¼‰
        if self.screen_perceptor is not None:
            screenshot_file_info, perception_infos = await self.screen_perceptor.get_perception_infos(
                screenshot_file_info,
                ui_hierarchy_xml,
                non_visual_mode=self.config.non_visual_mode,
                target_app=activity_info.package_name
            )
            perception_infos.keyboard_status = keyboard_status[1] == "true"

            # è°ƒè¯•ï¼šæ‰“å°æ ‡è®°æ˜ å°„å’Œä¿å­˜æ ‡è®°å›¾åƒ
            if perception_infos.use_set_of_marks_mapping and perception_infos.SoM_mapping:
                print(f"\nğŸ“ [DEBUG] å±å¹•æ ‡è®°æ˜ å°„ï¼ˆæ‰€æœ‰æ ‡è®°ï¼‰:")
                for mark_num in sorted(perception_infos.SoM_mapping.keys()):
                    coords = perception_infos.SoM_mapping[mark_num]
                    print(f"  æ ‡è®° #{mark_num} -> åæ ‡ {coords}")

                # ä¿å­˜æ ‡è®°åçš„å›¾åƒç”¨äºè°ƒè¯•
                import shutil
                marked_image_path = screenshot_file_info.get_screenshot_fullpath()  # ä¿®å¤ï¼šä½¿ç”¨fullpathè€Œä¸æ˜¯Imageå¯¹è±¡
                debug_image_path = f"{self.config.temp_path}/debug_marked_screen.jpg"
                shutil.copy(marked_image_path, debug_image_path)
                print(f"\nğŸ–¼ï¸  [DEBUG] æ ‡è®°åçš„å›¾åƒå·²ä¿å­˜åˆ°: {debug_image_path}")
                print(f"    è¯·æŸ¥çœ‹å›¾åƒï¼Œç¡®è®¤åº•éƒ¨å¯¼èˆªæ çš„æ ‡è®°å·æ˜¯å¦æ­£ç¡®")
                print()
        else:
            # å¦‚æœæ²¡æœ‰æ„ŸçŸ¥å™¨ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ„ŸçŸ¥ä¿¡æ¯
            from Fairy.tools.screen_perceptor.ssip_new.perceptor.entity import SSIPInfo
            perception_infos = SSIPInfo(
                width=1080,
                height=1920,
                perception_infos=[ui_hierarchy_xml, None],
                non_visual_mode=True,
                SoM_mapping=None
            )
            perception_infos.keyboard_status = keyboard_status[1] == "true"

        return ScreenInfo(screenshot_file_info, perception_infos, activity_info)

    async def _decide_action(
        self,
        instruction: str,
        language: str,
        plan_info: PlanInfo,
        screen_info: ScreenInfo,
        historical_actions: List[Dict],
        execution_tips: str,
        key_infos: List
    ) -> Optional[ActionInfo]:
        """
        ä½¿ç”¨LLMå†³ç­–åŠ¨ä½œ

        è¿™æ˜¯æ ¸å¿ƒæ–¹æ³•ï¼Œå¤ç”¨äº†Fairyçš„AppActionDeciderAgentçš„é€»è¾‘
        """
        print(f"\nğŸ¤– [DEBUG] å¼€å§‹LLMå†³ç­–...")
        print(f"   æŒ‡ä»¤: {instruction}")

        # æ„å»ºPrompt
        prompt = self._build_action_decision_prompt(
            instruction=instruction,
            language=language,
            plan_info=plan_info,
            screen_info=screen_info,
            historical_actions=historical_actions,
            execution_tips=execution_tips,
            key_infos=key_infos
        )
        print(f"   Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")

        # å‡†å¤‡å›¾åƒ
        images = []
        if not self.config.non_visual_mode:
            images.append(screen_info.screenshot_file_info.get_screenshot_Image_file())
            print(f"   ä½¿ç”¨è§†è§‰æ¨¡å¼ï¼Œå›¾åƒè·¯å¾„: {images[0]}")
        else:
            print(f"   ä½¿ç”¨éè§†è§‰æ¨¡å¼ï¼ˆçº¯æ–‡æœ¬ï¼‰")

        # æ„å»ºæ¶ˆæ¯
        system_message = ChatMessage(
            content="You are part of a helpful AI assistant for operating mobile phones and your identity is an action decider. Your goal is to choose the correct atomic actions to complete the user's instruction. Think as if you are a human user operating the phone.",
            type="SystemMessage"
        )

        user_message = ChatMessage(
            content=[prompt] + images,
            type="UserMessage",
            source="user"
        )

        # è°ƒç”¨LLM
        print(f"   æ­£åœ¨è°ƒç”¨LLM...")
        try:
            response = await self.model_client.create([system_message, user_message])
            print(f"   LLMå“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
            print(f"   LLMå“åº”å†…å®¹:\n{response.content}\n")
        except Exception as e:
            print(f"âŒ [ERROR] LLMè°ƒç”¨å¤±è´¥: {e}")
            return None

        # è§£æå“åº”
        print(f"   æ­£åœ¨è§£æLLMå“åº”...")
        action_info = self._parse_action_response(response.content, screen_info)

        if action_info is None:
            print(f"âŒ [ERROR] è§£æLLMå“åº”å¤±è´¥ï¼Œè¿”å›None")
        else:
            print(f"âœ… [DEBUG] LLMå†³ç­–æˆåŠŸ")

        return action_info

    def _build_action_decision_prompt(
        self,
        instruction: str,
        language: str,
        plan_info: PlanInfo,
        screen_info: ScreenInfo,
        historical_actions: List[Dict],
        execution_tips: str,
        key_infos: List
    ) -> str:
        """
        æ„å»ºåŠ¨ä½œå†³ç­–çš„Prompt

        å¤ç”¨Fairyçš„AppActionDeciderAgent.build_prompté€»è¾‘
        """
        # åŸºæœ¬ä¿¡æ¯
        prompt = f"---\n" \
                 f"- Instruction: {instruction}\n" \
                 f"- Overall Plan: {plan_info.overall_plan}\n" \
                 f"- Current Sub-goal: {plan_info.current_sub_goal}\n" \
                 f"- Key Information Record (Excluding Current Screen): {key_infos}\n" \
                 f"\n"

        # å±å¹•ä¿¡æ¯
        prompt += f"---\n"
        if not self.config.non_visual_mode:
            screenshot_prompt = "The attached image is a screenshots of your phone to show the current state"
        else:
            screenshot_prompt = "The following text description (e.g. JSON or XML) is converted from a screenshots of your phone to show the current state"

        prompt += screen_info.perception_infos.get_screen_info_note_prompt(screenshot_prompt)
        prompt += f"\n"
        prompt += screen_info.perception_infos.get_screen_info_prompt()

        prompt += f"Please scrutinize the above screen information to infer the type of the current page (e.g., home page, search page, results page, details page, etc.) and thus the main function of the page. This helps you to avoid wrong actions.\n"

        # åŠ¨ä½œé€‰æ‹©æŒ‡å¯¼
        prompt += "---\n"
        prompt += "Carefully examine all the information provided above and decide on the next action to perform. If you notice an unsolved error in the previous action, think as a human user and attempt to rectify them. You must choose your action from ONE or MORE of the atomic actions.\n"
        prompt += "If there are multiple options and the user does not specify which one to choose in the Instruction, interaction with the user is necessary. You cannot make any choices on behalf of the user.\n"
        prompt += "\n"

        # åŸå­åŠ¨ä½œåˆ—è¡¨
        prompt += "- Atomic Actions: \n"
        prompt += "The atomic action functions are listed in the format of `name(arguments): description` as follows:\n"

        use_som = screen_info.perception_infos.use_set_of_marks_mapping
        for action, value in ATOMIC_ACTION_SIGNITURES.items():
            if use_som:
                prompt += f"- {action}({', '.join(value['SoM_arguments'])}): {value['description'](True)}\n"
            else:
                prompt += f"- {action}({', '.join(value['arguments'])}): {value['description'](False)}\n"

        prompt += f"IMPORTANT: When you input something (especially a search), please be careful to use the language {language}.\n\n"

        if not screen_info.perception_infos.keyboard_status:
            prompt += "NOTE: Unable to input. The keyboard has not been activated. To input, please activate the keyboard by tapping on an input box, which includes tapping on an input box first.\n\n"

        # å†å²åŠ¨ä½œ
        prompt += f"---\n- Latest Action History: \n"
        if len(historical_actions) > 0:
            prompt += "(Recent actions you took previously)\n"
            for action in historical_actions[-5:]:  # æœ€è¿‘5ä¸ª
                prompt += f"Action: {action}\n"
            prompt += "\n"
        else:
            prompt += "No actions have been taken yet.\n\n"

        # æ‰§è¡ŒTips
        if execution_tips:
            prompt += f"---\n"
            prompt += f"Here's some TIPS for execution the action. These TIPS are VERY IMPORTANT, so MAKE SURE you follow them to the letter!\n"
            prompt += f"{execution_tips}\n\n"

        # è¾“å‡ºæ ¼å¼
        prompt += "---\n"
        prompt += "Please provide a JSON with 4 keys, which are interpreted as follows:\n"
        prompt += "- action_thought: A detailed explanation of your rationale for the chosen action.\n"
        prompt += "- actions: ONE or MORE action from the 'Atomic Actions' provided. IMPORTANT: DO NOT return invalid actions like null or stop. DO NOT repeat previously failed actions. The decided action must be provided in a valid JSON format and should be an array containing a sequence of actions, specifying the name and parameters of the action. For example, if you decide to tap on position (100, 200) first, you should first put in the array {\"name\":\"Tap\", \"arguments\":{\"x\":100, \"y\":100}}. If an action does not require parameters, such as 'Wait', fill in the 'Parameters' field with null. IMPORTANT: MAKE SURE the parameter key matches the signature of the action function exactly. MAKE SURE that the order of the actions in the array is the same as the order in which you want them to be executed. MAKE SURE this JSON can be loaded correctly by json.load().\n"
        prompt += "- action_expectation: A brief description of the expected results of the selected action(s).\n"
        prompt += "- user_interaction_thought: A judgment on whether or not need to interact with the user and explain the reasons.\n"
        prompt += "Make sure this JSON can be loaded correctly by json.load().\n"

        return prompt

    def _parse_action_response(self, response: str, screen_info: ScreenInfo) -> Optional[ActionInfo]:
        """
        è§£æLLMçš„åŠ¨ä½œå†³ç­–å“åº”

        å¤ç”¨Fairyçš„AppActionDeciderAgent.parse_responseé€»è¾‘
        """
        try:
            # æå–JSON
            if "json" in response:
                response = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL).group(1)

            response_json = json.loads(response)

            # éªŒè¯åŠ¨ä½œ
            for action in response_json['actions']:
                if action['name'] not in [action_type.value for action_type in AtomicActionType]:
                    print(f"Error! Invalid action name: {action['name']}")
                    return None

            # SoMåæ ‡è½¬æ¢
            actions = response_json['actions']
            if screen_info.perception_infos.use_set_of_marks_mapping:
                print(f"ğŸ” [DEBUG] LLMè¿”å›çš„åŸå§‹åŠ¨ä½œï¼ˆå¸¦æ ‡è®°å·ï¼‰: {actions}")
                actions = self._convert_som_to_coordinates(
                    actions,
                    screen_info.perception_infos.convert_marks_to_coordinates
                )
                print(f"âœ… [DEBUG] è½¬æ¢åçš„åŠ¨ä½œï¼ˆå¸¦åæ ‡ï¼‰: {actions}")

            return ActionInfo(
                action_thought=response_json['action_thought'],
                actions=actions,
                action_expectation=response_json['action_expectation'],
                user_interaction_thought=response_json['user_interaction_thought']
            )

        except Exception as e:
            print(f"Failed to parse action response: {e}")
            print(f"Response: {response}")
            return None

    def _convert_som_to_coordinates(self, actions: List[Dict], convert_func) -> List[Dict]:
        """
        å°†Set-of-Marksæ ‡è®°å·è½¬æ¢ä¸ºåæ ‡

        å¤ç”¨Fairyçš„AppActionDeciderAgent.SoM_args_conversioné€»è¾‘
        """
        converted_actions = []

        for action in actions:
            action_type = AtomicActionType(action['name'])

            if action_type == AtomicActionType.Tap:
                mark_number = action['arguments']['mark_number']
                coordinate = convert_func(mark_number)
                print(f"ğŸ”„ [DEBUG] è½¬æ¢æ ‡è®° #{mark_number} -> åæ ‡ {coordinate}")
                if coordinate:
                    converted_actions.append({
                        'name': action['name'],
                        'arguments': {'x': coordinate[0], 'y': coordinate[1]}
                    })
                else:
                    print(f"âš ï¸  [WARNING] æ ‡è®° #{mark_number} è½¬æ¢å¤±è´¥ï¼åæ ‡ä¸ºNoneï¼ŒåŠ¨ä½œè¢«ä¸¢å¼ƒ")

            elif action_type == AtomicActionType.LongPress:
                mark_number = action['arguments']['mark_number']
                coordinate = convert_func(mark_number)
                print(f"ğŸ”„ [DEBUG] è½¬æ¢æ ‡è®° #{mark_number} -> åæ ‡ {coordinate}")
                if coordinate:
                    converted_actions.append({
                        'name': action['name'],
                        'arguments': {
                            'x': coordinate[0],
                            'y': coordinate[1],
                            'duration': action['arguments']['duration']
                        }
                    })
                else:
                    print(f"âš ï¸  [WARNING] æ ‡è®° #{mark_number} è½¬æ¢å¤±è´¥ï¼åæ ‡ä¸ºNoneï¼ŒåŠ¨ä½œè¢«ä¸¢å¼ƒ")

            elif action_type == AtomicActionType.Swipe:
                mark_number = action['arguments']['mark_number']
                bounds = convert_func(mark_number)
                print(f"ğŸ”„ [DEBUG] è½¬æ¢æ ‡è®° #{mark_number} -> è¾¹ç•Œ {bounds}")
                if bounds:
                    (x1, y1), (x2, y2) = bounds
                    center_x = (x1 + x2) / 2
                    center_y = (y1 + y2) / 2
                    width = x2 - x1
                    height = y2 - y1
                    distance = action['arguments']['distance']
                    duration = action['arguments']['duration']
                    direction = action['arguments']['direction']

                    if direction == 'H':
                        dy = height * abs(distance) / 2
                        start_y = center_y + dy if distance > 0 else center_y - dy
                        end_y = center_y - dy if distance > 0 else center_y + dy
                        converted_actions.append({
                            'name': action['name'],
                            'arguments': {
                                'x1': center_x, 'y1': start_y,
                                'x2': center_x, 'y2': end_y,
                                'duration': duration
                            }
                        })
                    elif direction == 'W':
                        dx = width * abs(distance) / 2
                        start_x = center_x + dx if distance > 0 else center_x - dx
                        end_x = center_x - dx if distance > 0 else center_x + dx
                        converted_actions.append({
                            'name': action['name'],
                            'arguments': {
                                'x1': start_x, 'y1': center_y,
                                'x2': end_x, 'y2': center_y,
                                'duration': duration
                            }
                        })
                else:
                    print(f"âš ï¸  [WARNING] æ ‡è®° #{mark_number} è½¬æ¢å¤±è´¥ï¼è¾¹ç•Œä¸ºNoneï¼ŒåŠ¨ä½œè¢«ä¸¢å¼ƒ")
            else:
                # å…¶ä»–åŠ¨ä½œä¸éœ€è¦è½¬æ¢
                converted_actions.append(action)

        print(f"ğŸ“Š [DEBUG] è½¬æ¢ç»“æœ: {len(actions)} ä¸ªåŸå§‹åŠ¨ä½œ -> {len(converted_actions)} ä¸ªè½¬æ¢åçš„åŠ¨ä½œ")
        return converted_actions

    async def _execute_actions(self, actions: List[Dict]) -> None:
        """æ‰§è¡ŒåŠ¨ä½œåºåˆ—"""
        if not actions:
            print(f"âŒ [ERROR] åŠ¨ä½œåˆ—è¡¨ä¸ºç©ºï¼æ²¡æœ‰å¯æ‰§è¡Œçš„åŠ¨ä½œ")
            return

        print(f"ğŸ¯ [DEBUG] å‡†å¤‡æ‰§è¡Œ {len(actions)} ä¸ªåŠ¨ä½œ: {actions}")
        await self.controller.execute_actions(actions)
        print(f"âœ… [DEBUG] åŠ¨ä½œæ‰§è¡Œå®Œæˆ")

    # ==================== ä¾¿æ·æ–¹æ³• ====================

    async def get_current_screen(self) -> ScreenInfo:
        """è·å–å½“å‰å±å¹•ä¿¡æ¯ï¼ˆä¸æ‰§è¡ŒåŠ¨ä½œï¼‰"""
        return await self._get_screen_info()

    async def execute_raw_actions(self, actions: List[Dict]) -> None:
        """ç›´æ¥æ‰§è¡ŒåŠ¨ä½œï¼ˆä¸ç»è¿‡LLMå†³ç­–ï¼‰"""
        await self._execute_actions(actions)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
async def non_visual_mode_usage():
    """éè§†è§‰æ¨¡å¼ - ä½¿ç”¨æ–‡æœ¬æè¿°è€Œä¸æ˜¯æ ‡è®°å›¾åƒ"""
    load_dotenv()

    core_model_client = OpenAIChatClient({
        "model": os.getenv("CORE_LMM_MODEL_NAME"),
        "api_key": os.getenv("CORE_LMM_API_KEY"),
        "base_url": os.getenv("CORE_LMM_API_BASE")
    })

    visual_model_config = ModelConfig(
        model_name=os.getenv("VISUAL_PROMPT_LMM_API_NAME"),
        model_temperature=0,
        model_info={"vision": True, "function_calling": False, "json_output": False},
        api_base=os.getenv("VISUAL_PROMPT_LMM_API_BASE"),
        api_key=os.getenv("VISUAL_PROMPT_LMM_API_KEY")
    )

    text_summary_config = ModelConfig(
        model_name=os.getenv("RAG_LLM_API_NAME"),
        model_temperature=0,
        model_info={"vision": False, "function_calling": False, "json_output": False},
        api_base=os.getenv("RAG_LLM_API_BASE"),
        api_key=os.getenv("RAG_LLM_API_KEY")
    )

    config = MinimalExecutorConfig(
        device=os.getenv("DEVICE_ID", "10.176.65.211:7421"),
        model_client=core_model_client,
        visual_prompt_model_config=visual_model_config,
        text_summarization_model_config=text_summary_config,
        non_visual_mode=True  # True=ä½¿ç”¨æ–‡æœ¬æè¿°æ¨¡å¼
    )

    executor = MinimalFairyExecutor(
        device=config.device,
        model_client=config.model_client,
        config=config
    )

    result = await executor.execute_instruction(
        instruction="è¿›å…¥æ¸¸æˆæ ç›®",
        plan_context={
            "overall_plan": "è¿›å…¥æ¸¸æˆæ ç›®",
            "current_sub_goal": "ç‚¹å‡»æ¸¸æˆtab"
        },
        execution_tips="åˆ†ç±»é€šå¸¸åœ¨ä¾§é¢æˆ–è€…ä¸‹é¢"
    )

    print(f"æ‰§è¡ŒæˆåŠŸ: {result.success}")
    print(f"æ‰§è¡Œçš„åŠ¨ä½œ: {result.actions_taken}")

    return result

async def correct_usage():
    """æ­£ç¡®çš„ä½¿ç”¨æ–¹å¼ - é…ç½®å±å¹•æ„ŸçŸ¥å™¨"""
    load_dotenv()

    # 1. åˆ›å»ºæ ¸å¿ƒæ¨¡å‹å®¢æˆ·ç«¯ï¼ˆç”¨äºåŠ¨ä½œå†³ç­–ï¼‰
    core_model_client = OpenAIChatClient({
        "model": os.getenv("CORE_LMM_MODEL_NAME"),
        "api_key": os.getenv("CORE_LMM_API_KEY"),
        "base_url": os.getenv("CORE_LMM_API_BASE")
    })

    # 2. é…ç½®è§†è§‰æ¨¡å‹ï¼ˆç”¨äºå±å¹•ç†è§£ï¼‰â­å…³é”®ï¼
    visual_model_config = ModelConfig(
        model_name=os.getenv("VISUAL_PROMPT_LMM_API_NAME"),
        model_temperature=0,
        model_info={"vision": True, "function_calling": False, "json_output": False},
        api_base=os.getenv("VISUAL_PROMPT_LMM_API_BASE"),
        api_key=os.getenv("VISUAL_PROMPT_LMM_API_KEY")
    )

    # 3. é…ç½®æ–‡æœ¬æ‘˜è¦æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºnon_visual_modeï¼‰
    text_summary_config = ModelConfig(
        model_name=os.getenv("RAG_LLM_API_NAME"),
        model_temperature=0,
        model_info={"vision": False, "function_calling": False, "json_output": False},
        api_base=os.getenv("RAG_LLM_API_BASE"),
        api_key=os.getenv("RAG_LLM_API_KEY")
    )

    # 4. åˆ›å»ºå®Œæ•´é…ç½®
    config = MinimalExecutorConfig(
        device=os.getenv("DEVICE_ID", "10.176.65.211:7421"),
        model_client=core_model_client,
        visual_prompt_model_config=visual_model_config,  # â­å¿…é¡»é…ç½®
        text_summarization_model_config=text_summary_config,
        non_visual_mode=False  # False=ä½¿ç”¨Set-of-Marksæ ‡è®°æ¨¡å¼ï¼ˆæ¨èï¼‰
    )

    # 5. åˆ›å»ºæ‰§è¡Œå™¨
    executor = MinimalFairyExecutor(
        device=config.device,
        model_client=config.model_client,
        config=config
    )

    # 6. æ‰§è¡ŒæŒ‡ä»¤
    result = await executor.execute_instruction(
        instruction="é¢„çº¦12æœˆ12å·æ‰ä¸Šçº¿çš„æ–°æ¸¸æˆ",
        plan_context={
            "overall_plan": "æ‰¾åˆ°12æœˆ12å·çš„æ¸¸æˆ",
            "current_sub_goal": "ç‚¹å‡»12-12"
        },
        execution_tips=""
    )

    # result = await executor.execute_instruction(
    #     instruction="è´­ä¹°ä¸€ä¸ªéº¦è¾£é¸¡è…¿å ¡",
    #     plan_context={
    #         "overall_plan": "è¿›å…¥é¸¡è…¿æ±‰å ¡çš„æ ç›®",
    #         "current_sub_goal": "ç‚¹å‡»é¸¡è…¿æ±‰å ¡/å·"
    #     },
    #     execution_tips=""
    # )

    print(f"æ‰§è¡ŒæˆåŠŸ: {result.success}")
    print(f"æ‰§è¡Œçš„åŠ¨ä½œ: {result.actions_taken}")
    print(f"åŠ¨ä½œæ€è€ƒ: {result.action_thought}")
    print(f"é¢„æœŸç»“æœ: {result.action_expectation}")
    if result.error:
        print(f"âŒ é”™è¯¯ä¿¡æ¯: {result.error}")

    return result

if __name__ == "__main__":
    asyncio.run(correct_usage())
